# 如何逐步構建網站爬蟲程式：完整 Prompt 步驟指南

根據我們的討論和所有模組的開發，以下是逐步構建網站爬蟲程式的詳細 Prompt 步驟。這個指南涵蓋了從基礎架構到高級功能的完整開發流程。

## 第一階段：定義基本需求和架構

**Prompt 1: 基本需求定義**
```
請設計一個爬取網站的爬蟲程式，需要實現以下功能：
- 使用 Selenium 進行網頁自動化
- 能夠處理登入機制
- 需要防止被網站封鎖
- 要有錯誤處理機制
- 需要記錄日誌
- 支援數據保存功能
```

**Prompt 2: 專案架構設計**
```
請為這個爬蟲專案設計合適的檔案結構，包括：
- 主程式檔案
- Cookie 管理模組
- 爬蟲核心運轉模組
- 錯誤處理模組
- 設定檔
- 數據處理與儲存模組

並說明每個模組的主要功能和責任。
```

## 第二階段：核心功能設計

**Prompt 3: 爬蟲核心功能**
```
請設計爬蟲的核心功能流程：
1. WebDriver 配置與初始化
2. Cookie 管理機制
3. 登入處理流程
4. 數據爬取邏輯
5. 錯誤重試機制

每個功能都需要：
- 適當的錯誤處理
- 詳細的日誌記錄
- 參數可配置性
```

**Prompt 4: 反爬蟲機制**
```
請設計一個全面的反爬蟲防禦類別 (AntiDetectionManager)，包含以下功能：
1. 隨機 User-Agent 切換
2. 代理伺服器支援
3. WebDriver 特徵隱藏
4. 隨機等待時間
5. Cookie 管理
6. 爬取頻率限制

這個類應該能夠：
- 檢測各種反爬蟲機制
- 記錄可能的反爬蟲原因
- 在爬蟲中斷時儲存狀態
- 根據不同的檢測類型採取相應的防禦措施
```

## 第三階段：模板化設計

**Prompt 5: 模板化爬蟲設計**
```
不同的網站其實爬取資料的流程邏輯都接近：
1. 取得列表
2. 取得下一列表分頁
3. 取得內容

重點在取得列表、分頁、內容的 HTML XPath，可參考 Scrapy 的作法。

可否將這 3 個步驟的 XPath 及解讀程序，以 JSON 格式儲存在一個 templates 中？
如此，即能用同樣的程式爬取不同的網站。
```

**Prompt 6: GET 參數定義與模板設計**
```
爬蟲 JSON 範本裡應加入 GET 方法後面的參數定義，例如：
- 固定參數：常用的查詢條件
- 可變參數：需要動態生成的參數
- 分頁參數：控制分頁的參數

請提供一個完整的網站爬蟲模板示例，包含 URL 參數、XPath 選擇器和延遲設定。
```

## 第四階段：斷點續爬與狀態管理

**Prompt 7: 狀態管理與斷點續爬**
```
在資料數據儲存的部份，應有一資料表或 collection 能記錄各爬蟲的最近一次爬蟲名稱(含id)及狀態，例：完成；中斷；失敗等，並記錄時間及中斷節點，若是中斷，使下次啟動執行時能從此開始。

請設計一個爬蟲任務管理器，能夠記錄和管理爬蟲任務的狀態。
```

**Prompt 8: 多重備份狀態管理**
```
同意使用多重備份：為了提高可靠性，建議同時使用多種存儲方式，例如：
- Local Storage 用於短期狀態恢復
- 文件系統用於長期數據存儲
- 數據庫用於結構化數據管理

請實現一個多重備份爬蟲狀態管理器。
```

## 第五階段：數據持久化

**Prompt 9: 靈活的數據結構設計**
```
上述的資料庫定義，能使用 meta field 的思維，保留彈性，以適應不同的網站內容需要的欄位。

請設計一個彈性化的 MongoDB Collection 結構和對應的 Notion 資料庫結構，使用以下特點：
- 核心通用欄位保持固定
- 使用 data 子文檔存放核心業務數據
- 使用 metadata 子文檔存放額外信息
- 支持動態欄位映射
```

**Prompt 10: 數據持久化模塊**
```
請設計一個數據持久化模塊，支援以下功能：
1. 保存數據到本地文件系統
2. 保存數據到 MongoDB
3. 保存數據到 Notion
4. 支援各儲存位置之間的數據同步
5. 支援可配置的字段映射

同時，需要一個安全的配置載入工具來管理 MongoDB 和 Notion 的憑證。
```

## 第六階段：驗證碼處理與高級反爬

**Prompt 11: 圖形驗證碼處理模組**
```
請設計一個用於處理各種圖形驗證碼的獨立模組，需要支援以下功能：
1. 文字型驗證碼處理
2. 滑塊型驗證碼處理
3. 點擊型驗證碼處理
4. 旋轉型驗證碼處理
5. Google ReCAPTCHA 處理

這個模組應該：
- 有良好的抽象設計，便於擴展新的驗證碼類型
- 支援保存驗證碼樣本以用於未來機器學習
- 能夠整合到爬蟲狀態系統中，記錄驗證碼相關的中斷
- 預留機器學習接入點，以便未來增強
```

**Prompt 12: 高級反爬技術與人類行為模擬**
```
除了基本的反爬蟲技術外，請添加以下高級功能：
1. 瀏覽器指紋修改（Canvas, WebGL, Audio）
2. 模擬真實人類行為（滾動模式、停頓、鼠標移動）
3. JS 執行環境隱藏
4. HTTP 標頭定制

同時加強檢測功能，能夠識別：
- 蜜罐陷阱和隱形元素
- JavaScript 指紋檢測
- 行為分析檢測
- 異常請求頻率警告
```

## 第七階段：整合與測試

**Prompt 13: 完整系統整合**
```
請整合所有之前開發的模組，實現一個完整的模板化爬蟲系統。這個系統應該能夠：
1. 通過 JSON 模板配置爬取不同網站
2. 支援斷點續爬功能
3. 處理反爬蟲檢測和驗證碼挑戰
4. 將數據保存到多個位置
5. 提供詳細的日誌和錯誤處理
6. 支援命令行參數配置

同時提供一個示例，展示如何用此系統爬取網站。
```

**Prompt 14: 測試與調試指南**
```
請提供爬蟲系統的測試與調試指南，包括：
1. 如何測試模板的 XPath 選擇器
2. 如何調試爬蟲過程中的問題
3. 如何驗證數據的完整性
4. 如何分析和解決常見的反爬蟲與驗證碼問題
5. 如何優化爬蟲性能
```

## 第八階段：文檔與部署

**Prompt 15: 系統文檔與代碼管理**
```
請依據以下架構撰寫完整的系統文檔：

1. 系統架構
   - 技術棧概述
   - 系統組件關係圖
   - 數據流程圖

2. 詳細設計文檔
   - 每個模組的類圖
   - 核心流程的時序圖
   - 關鍵算法說明

3. 使用指南
   - 環境設定步驟
   - 配置文件格式說明
   - 命令行參數說明
   - 常見問題解答

4. API 文檔
   - RESTful API 接口定義
   - 各類別的公開方法說明
   - 錯誤代碼對照表

5. 開發指南
   - 代碼風格規範
   - Git 工作流程
   - 單元測試規範
   - 發布流程說明
```

**Prompt 16: 部署與維運管理**
```
請提供一個完整的部署與維運方案：

1. 容器化部署
   - Dockerfile 配置
   - Docker Compose 設定
   - 容器資源限制設定
   - 多容器協作策略

2. 自動化運維
   - CI/CD 流程設計
   - 自動化測試整合
   - 定時任務配置
   - 日誌輪轉策略

3. 監控與告警
   - 系統性能指標監控
   - 爬蟲任務狀態監控
   - 異常檢測規則
   - 告警層級與通知機制

4. 錯誤處理
   - 自動重試機制
   - 錯誤分類與處理策略
   - 緊急停止機制
   - 故障恢復流程

5. 數據管理
   - 數據備份策略
   - 數據同步機制
   - 數據清理政策
   - 數據質量監控
```

通過這 16 個精心設計的 Prompt 步驟，您將能夠建立一個完整的爬蟲系統。這個系統不只是一個簡單的爬蟲程式，而是一個企業級的數據採集解決方案，具備完整的開發文檔、部署方案和運維管理體系。

每個 Prompt 都經過優化，確保開發過程的連貫性和完整性。這種結構化的方法不僅幫助開發團隊更好地理解和實現系統，也為後續的維護和擴展提供了堅實的基礎。

注意：在實際開發中，可能需要根據具體項目需求和團隊情況對這些 Prompt 進行適當調整。持續的反饋和優化是確保系統成功的關鍵。